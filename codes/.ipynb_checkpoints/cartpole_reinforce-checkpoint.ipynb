{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_states (InputLayer)       [(None, 1, 4)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 12)        60          input_states[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_action_matrixs (InputLaye [(None, 1, 2)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rewards (InputLayer)      [(None, 1, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1, 2)         26          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "episode:0, moving_avg:14.0, rewards_avg:13.0\n",
      "episode:10, moving_avg:36.81818181818182, rewards_avg:35.81818181818182\n",
      "episode:20, moving_avg:46.6, rewards_avg:44.04761904761905\n",
      "episode:30, moving_avg:46.65, rewards_avg:42.16129032258065\n",
      "episode:40, moving_avg:51.05, rewards_avg:46.97560975609756\n",
      "episode:50, moving_avg:64.3, rewards_avg:50.450980392156865\n",
      "episode:60, moving_avg:71.75, rewards_avg:54.77049180327869\n",
      "episode:70, moving_avg:77.9, rewards_avg:57.901408450704224\n",
      "episode:80, moving_avg:79.5, rewards_avg:60.629629629629626\n",
      "episode:90, moving_avg:67.5, rewards_avg:59.79120879120879\n",
      "episode:100, moving_avg:68.85, rewards_avg:62.05940594059406\n",
      "episode:110, moving_avg:73.55, rewards_avg:62.090090090090094\n",
      "episode:120, moving_avg:68.55, rewards_avg:62.96694214876033\n",
      "episode:130, moving_avg:85.95, rewards_avg:65.58015267175573\n",
      "episode:140, moving_avg:114.05, rewards_avg:70.0709219858156\n",
      "episode:150, moving_avg:118.25, rewards_avg:72.42384105960265\n",
      "episode:160, moving_avg:99.65, rewards_avg:73.62111801242236\n",
      "episode:170, moving_avg:88.8, rewards_avg:74.22222222222223\n",
      "episode:180, moving_avg:94.45, rewards_avg:75.8121546961326\n",
      "episode:190, moving_avg:134.3, rewards_avg:80.40837696335079\n",
      "episode:200, moving_avg:139.1, rewards_avg:82.00995024875621\n",
      "episode:210, moving_avg:113.3, rewards_avg:83.43127962085308\n",
      "episode:220, moving_avg:121.2, rewards_avg:85.46606334841628\n",
      "episode:230, moving_avg:123.9, rewards_avg:86.84848484848484\n",
      "episode:240, moving_avg:116.2, rewards_avg:87.93360995850622\n",
      "episode:250, moving_avg:104.45, rewards_avg:88.17131474103586\n",
      "episode:260, moving_avg:96.7, rewards_avg:88.52873563218391\n",
      "episode:270, moving_avg:103.25, rewards_avg:89.21033210332104\n",
      "episode:280, moving_avg:121.25, rewards_avg:90.7864768683274\n",
      "episode:290, moving_avg:113.35, rewards_avg:90.80068728522336\n",
      "episode:300, moving_avg:101.3, rewards_avg:91.4186046511628\n",
      "episode:310, moving_avg:114.8, rewards_avg:92.27974276527331\n",
      "episode:320, moving_avg:142.2, rewards_avg:94.5202492211838\n",
      "episode:330, moving_avg:166.4, rewards_avg:96.69788519637463\n",
      "episode:340, moving_avg:220.2, rewards_avg:101.83284457478005\n",
      "episode:350, moving_avg:251.45, rewards_avg:105.45868945868946\n",
      "episode:360, moving_avg:210.7, rewards_avg:107.80886426592798\n",
      "episode:370, moving_avg:169.95, rewards_avg:108.88140161725067\n",
      "episode:380, moving_avg:152.35, rewards_avg:110.09448818897638\n",
      "episode:390, moving_avg:154.05, rewards_avg:111.14066496163683\n",
      "episode:400, moving_avg:140.65, rewards_avg:111.56857855361596\n",
      "episode:410, moving_avg:141.25, rewards_avg:112.55717761557177\n",
      "episode:420, moving_avg:156.2, rewards_avg:113.64133016627079\n",
      "episode:430, moving_avg:139.45, rewards_avg:113.75870069605568\n",
      "episode:440, moving_avg:121.85, rewards_avg:113.96825396825396\n",
      "episode:450, moving_avg:122.05, rewards_avg:114.0820399113082\n",
      "episode:460, moving_avg:123.7, rewards_avg:114.3470715835141\n",
      "episode:470, moving_avg:113.3, rewards_avg:114.00636942675159\n",
      "episode:480, moving_avg:95.45, rewards_avg:113.51975051975052\n",
      "episode:490, moving_avg:98.5, rewards_avg:113.33401221995926\n",
      "WARNING:tensorflow:From c:\\python377\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/reinforce\\assets\n",
      "*****end learing\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.node_num = 12\n",
    "        self.learning_rate = 0.0005\n",
    "        self.epochs_cnt = 5\n",
    "        self.model = self.build_model()        \n",
    "        \n",
    "        self.discount_rate = 0.95\n",
    "        self.penalty = -10\n",
    "\n",
    "        self.episode_num = 500\n",
    "        \n",
    "        self.moving_avg_size = 20\n",
    "        \n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "        self.states, self.action_matrixs, self.action_probs, self.rewards = [],[],[],[]\n",
    "        \n",
    "        self.DUMMY_ACTION_MATRIX, self.DUMMY_REWARD = np.zeros((1,1,self.action_size)), np.zeros((1,1,self.value_size))\n",
    "    \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_actions = data\n",
    "            states, action_matrix, rewards = in_datas[0], in_datas[1], in_datas[2]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                action_probs = K.sum(action_matrix*y_pred, axis=-1)\n",
    "                loss = -K.log(action_probs)*rewards\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
    "        input_rewards = Input(shape=(1,self.value_size), name='input_rewards')\n",
    "        \n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='tanh')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_rewards], outputs=out_actions)\n",
    "        \n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            self.env.max_episode_steps = 500\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "                \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "                \n",
    "            if(episode % 10 == 0):\n",
    "                print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))\n",
    "            \n",
    "        self.save_model()\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(state,[1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
    "            \n",
    "            action_prob = self.model.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_REWARD])\n",
    "            \n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size)\n",
    "            action_matrix[action] = 1\n",
    "            \n",
    "            state_next, reward, done, none = self.env.step(action)\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "            \n",
    "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "        return count, reward_tot\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states, self.action_matrixs, self.action_probs, self.rewards = [],[],[],[] #clear memory\n",
    "\n",
    "\n",
    "    def make_discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros(np.array(rewards).shape)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_rate + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        \n",
    "        return discounted_rewards\n",
    "    \n",
    "    def train_mini_batch(self):\n",
    "        discount_rewards = np.array(self.make_discount_rewards(self.rewards))\n",
    "        discount_rewards_t = np.reshape(discount_rewards, [len(discount_rewards),1,1])\n",
    "\n",
    "        states_t = np.array(self.states)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "\n",
    "        self.model.fit(x=[states_t, action_matrixs_t, discount_rewards_t], y=[action_probs_t], epochs=self.epochs_cnt, verbose=0)\n",
    "\n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save(\"./model/reinforce\")\n",
    "        print(\"*****end learing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(agent.reward_list, label='rewards')\n",
    "plt.plot(agent.moving_avg_list, linewidth=4, label='moving average')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('REINFORCE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.array([[0.6,0.4],\n",
    "                   [0.3,0.7]])\n",
    "action_matrix = np.array([[1,0],\n",
    "                          [0,1]])\n",
    "\n",
    "action_probs = K.sum(action_matrix*y_pred, axis=-1)\n",
    "print(\"*action_probs:\", action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = (x*x)\n",
    "z = tape.gradient(y, x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b= zip([1, 2, 3], [4, 5, 6])\n",
    "c = list(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "acton_prob = [0.7, 0.3]\n",
    "for i in range(10):\n",
    "    d = np.random.choice(2, 1, p=acton_prob)[0]\n",
    "    print(d, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "for t in reversed(range(0, len(a))):\n",
    "    print(t, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
