{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | discou... | epochs... | epochs... | layer_... | layer_... | learni... | learni... | loss_c... | mini_b... | node_n... | node_n... |  penalty  | smooth... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 37.65   \u001b[0m | \u001b[0m 0.9375  \u001b[0m | \u001b[0m 5.161   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 1.302   \u001b[0m | \u001b[0m 1.147   \u001b[0m | \u001b[0m 0.000183\u001b[0m | \u001b[0m 0.000267\u001b[0m | \u001b[0m 0.1691  \u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 74.5    \u001b[0m | \u001b[0m 60.63   \u001b[0m | \u001b[0m-164.2   \u001b[0m | \u001b[0m 0.9184  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 23.1    \u001b[0m | \u001b[0m 0.979   \u001b[0m | \u001b[0m 3.082   \u001b[0m | \u001b[0m 5.011   \u001b[0m | \u001b[0m 1.417   \u001b[0m | \u001b[0m 1.559   \u001b[0m | \u001b[0m 0.000226\u001b[0m | \u001b[0m 0.000278\u001b[0m | \u001b[0m 0.2601  \u001b[0m | \u001b[0m 77.59   \u001b[0m | \u001b[0m 48.36   \u001b[0m | \u001b[0m 92.31   \u001b[0m | \u001b[0m-70.57   \u001b[0m | \u001b[0m 0.9805  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 32.31   \u001b[0m | \u001b[0m 0.9077  \u001b[0m | \u001b[0m 3.117   \u001b[0m | \u001b[0m 3.509   \u001b[0m | \u001b[0m 1.878   \u001b[0m | \u001b[0m 1.098   \u001b[0m | \u001b[0m 0.000479\u001b[0m | \u001b[0m 0.000962\u001b[0m | \u001b[0m 0.2066  \u001b[0m | \u001b[0m 56.58   \u001b[0m | \u001b[0m 48.6    \u001b[0m | \u001b[0m 91.63   \u001b[0m | \u001b[0m-91.03   \u001b[0m | \u001b[0m 0.9016  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 25.11   \u001b[0m | \u001b[0m 0.9675  \u001b[0m | \u001b[0m 5.967   \u001b[0m | \u001b[0m 5.244   \u001b[0m | \u001b[0m 1.28    \u001b[0m | \u001b[0m 1.789   \u001b[0m | \u001b[0m 0.000192\u001b[0m | \u001b[0m 0.000503\u001b[0m | \u001b[0m 0.2817  \u001b[0m | \u001b[0m 26.31   \u001b[0m | \u001b[0m 45.38   \u001b[0m | \u001b[0m 27.08   \u001b[0m | \u001b[0m-490.5   \u001b[0m | \u001b[0m 0.9611  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 34.92   \u001b[0m | \u001b[0m 0.919   \u001b[0m | \u001b[0m 3.797   \u001b[0m | \u001b[0m 4.475   \u001b[0m | \u001b[0m 1.053   \u001b[0m | \u001b[0m 1.574   \u001b[0m | \u001b[0m 0.000232\u001b[0m | \u001b[0m 0.000630\u001b[0m | \u001b[0m 0.24    \u001b[0m | \u001b[0m 11.78   \u001b[0m | \u001b[0m 60.03   \u001b[0m | \u001b[0m 92.55   \u001b[0m | \u001b[0m-297.1   \u001b[0m | \u001b[0m 0.9045  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 143.0   \u001b[0m | \u001b[95m 0.9626  \u001b[0m | \u001b[95m 3.623   \u001b[0m | \u001b[95m 3.248   \u001b[0m | \u001b[95m 1.306   \u001b[0m | \u001b[95m 1.548   \u001b[0m | \u001b[95m 0.000779\u001b[0m | \u001b[95m 0.000576\u001b[0m | \u001b[95m 0.1574  \u001b[0m | \u001b[95m 51.44   \u001b[0m | \u001b[95m 42.72   \u001b[0m | \u001b[95m 87.71   \u001b[0m | \u001b[95m-87.47   \u001b[0m | \u001b[95m 0.9575  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 33.65   \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 3.826   \u001b[0m | \u001b[0m 5.102   \u001b[0m | \u001b[0m 1.779   \u001b[0m | \u001b[0m 1.849   \u001b[0m | \u001b[0m 0.000585\u001b[0m | \u001b[0m 0.000741\u001b[0m | \u001b[0m 0.2949  \u001b[0m | \u001b[0m 34.85   \u001b[0m | \u001b[0m 61.93   \u001b[0m | \u001b[0m 28.71   \u001b[0m | \u001b[0m-420.1   \u001b[0m | \u001b[0m 0.9347  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 261.0   \u001b[0m | \u001b[95m 0.9561  \u001b[0m | \u001b[95m 5.377   \u001b[0m | \u001b[95m 4.037   \u001b[0m | \u001b[95m 1.091   \u001b[0m | \u001b[95m 1.536   \u001b[0m | \u001b[95m 0.000949\u001b[0m | \u001b[95m 0.000489\u001b[0m | \u001b[95m 0.1358  \u001b[0m | \u001b[95m 17.45   \u001b[0m | \u001b[95m 90.82   \u001b[0m | \u001b[95m 122.4   \u001b[0m | \u001b[95m-133.1   \u001b[0m | \u001b[95m 0.9446  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 29.04   \u001b[0m | \u001b[0m 0.9352  \u001b[0m | \u001b[0m 3.343   \u001b[0m | \u001b[0m 5.206   \u001b[0m | \u001b[0m 1.199   \u001b[0m | \u001b[0m 1.062   \u001b[0m | \u001b[0m 0.000959\u001b[0m | \u001b[0m 0.000135\u001b[0m | \u001b[0m 0.2367  \u001b[0m | \u001b[0m 44.62   \u001b[0m | \u001b[0m 119.1   \u001b[0m | \u001b[0m 51.09   \u001b[0m | \u001b[0m-47.23   \u001b[0m | \u001b[0m 0.9792  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 23.04   \u001b[0m | \u001b[0m 0.9682  \u001b[0m | \u001b[0m 5.624   \u001b[0m | \u001b[0m 4.292   \u001b[0m | \u001b[0m 1.77    \u001b[0m | \u001b[0m 1.288   \u001b[0m | \u001b[0m 0.000258\u001b[0m | \u001b[0m 0.000582\u001b[0m | \u001b[0m 0.2948  \u001b[0m | \u001b[0m 36.58   \u001b[0m | \u001b[0m 83.53   \u001b[0m | \u001b[0m 23.39   \u001b[0m | \u001b[0m-362.3   \u001b[0m | \u001b[0m 0.9589  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 42.52   \u001b[0m | \u001b[0m 0.9226  \u001b[0m | \u001b[0m 3.466   \u001b[0m | \u001b[0m 5.953   \u001b[0m | \u001b[0m 1.183   \u001b[0m | \u001b[0m 1.364   \u001b[0m | \u001b[0m 0.000418\u001b[0m | \u001b[0m 0.000281\u001b[0m | \u001b[0m 0.2086  \u001b[0m | \u001b[0m 10.69   \u001b[0m | \u001b[0m 119.6   \u001b[0m | \u001b[0m 18.01   \u001b[0m | \u001b[0m-368.9   \u001b[0m | \u001b[0m 0.9539  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 50.81   \u001b[0m | \u001b[0m 0.9216  \u001b[0m | \u001b[0m 5.941   \u001b[0m | \u001b[0m 4.988   \u001b[0m | \u001b[0m 1.431   \u001b[0m | \u001b[0m 1.603   \u001b[0m | \u001b[0m 0.000164\u001b[0m | \u001b[0m 0.000936\u001b[0m | \u001b[0m 0.2232  \u001b[0m | \u001b[0m 17.56   \u001b[0m | \u001b[0m 75.3    \u001b[0m | \u001b[0m 126.5   \u001b[0m | \u001b[0m-123.4   \u001b[0m | \u001b[0m 0.9677  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 28.11   \u001b[0m | \u001b[0m 0.9274  \u001b[0m | \u001b[0m 3.438   \u001b[0m | \u001b[0m 5.431   \u001b[0m | \u001b[0m 1.271   \u001b[0m | \u001b[0m 1.792   \u001b[0m | \u001b[0m 0.000169\u001b[0m | \u001b[0m 0.000275\u001b[0m | \u001b[0m 0.2057  \u001b[0m | \u001b[0m 17.54   \u001b[0m | \u001b[0m 85.69   \u001b[0m | \u001b[0m 24.01   \u001b[0m | \u001b[0m-315.8   \u001b[0m | \u001b[0m 0.9555  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 15.64   \u001b[0m | \u001b[0m 0.9223  \u001b[0m | \u001b[0m 4.879   \u001b[0m | \u001b[0m 4.707   \u001b[0m | \u001b[0m 1.951   \u001b[0m | \u001b[0m 1.056   \u001b[0m | \u001b[0m 0.000329\u001b[0m | \u001b[0m 0.000266\u001b[0m | \u001b[0m 0.2299  \u001b[0m | \u001b[0m 61.55   \u001b[0m | \u001b[0m 55.41   \u001b[0m | \u001b[0m 118.3   \u001b[0m | \u001b[0m-390.9   \u001b[0m | \u001b[0m 0.928   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 17.65   \u001b[0m | \u001b[0m 0.9873  \u001b[0m | \u001b[0m 4.764   \u001b[0m | \u001b[0m 4.072   \u001b[0m | \u001b[0m 1.858   \u001b[0m | \u001b[0m 1.077   \u001b[0m | \u001b[0m 0.000435\u001b[0m | \u001b[0m 0.000464\u001b[0m | \u001b[0m 0.2783  \u001b[0m | \u001b[0m 39.85   \u001b[0m | \u001b[0m 55.34   \u001b[0m | \u001b[0m 21.24   \u001b[0m | \u001b[0m-484.9   \u001b[0m | \u001b[0m 0.9184  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 20.63   \u001b[0m | \u001b[0m 0.9692  \u001b[0m | \u001b[0m 3.7     \u001b[0m | \u001b[0m 5.935   \u001b[0m | \u001b[0m 1.737   \u001b[0m | \u001b[0m 1.245   \u001b[0m | \u001b[0m 0.000185\u001b[0m | \u001b[0m 0.000744\u001b[0m | \u001b[0m 0.1418  \u001b[0m | \u001b[0m 72.77   \u001b[0m | \u001b[0m 38.84   \u001b[0m | \u001b[0m 66.25   \u001b[0m | \u001b[0m-443.6   \u001b[0m | \u001b[0m 0.954   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 28.45   \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 4.063   \u001b[0m | \u001b[0m 5.433   \u001b[0m | \u001b[0m 1.973   \u001b[0m | \u001b[0m 1.702   \u001b[0m | \u001b[0m 0.000253\u001b[0m | \u001b[0m 0.000483\u001b[0m | \u001b[0m 0.2822  \u001b[0m | \u001b[0m 20.95   \u001b[0m | \u001b[0m 14.45   \u001b[0m | \u001b[0m 56.22   \u001b[0m | \u001b[0m-360.9   \u001b[0m | \u001b[0m 0.9325  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 31.68   \u001b[0m | \u001b[0m 0.9874  \u001b[0m | \u001b[0m 4.124   \u001b[0m | \u001b[0m 5.531   \u001b[0m | \u001b[0m 1.317   \u001b[0m | \u001b[0m 1.099   \u001b[0m | \u001b[0m 0.000648\u001b[0m | \u001b[0m 0.000143\u001b[0m | \u001b[0m 0.1877  \u001b[0m | \u001b[0m 64.63   \u001b[0m | \u001b[0m 66.42   \u001b[0m | \u001b[0m 46.25   \u001b[0m | \u001b[0m-324.1   \u001b[0m | \u001b[0m 0.9804  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 112.5   \u001b[0m | \u001b[0m 0.9468  \u001b[0m | \u001b[0m 5.848   \u001b[0m | \u001b[0m 4.31    \u001b[0m | \u001b[0m 1.338   \u001b[0m | \u001b[0m 1.942   \u001b[0m | \u001b[0m 0.000230\u001b[0m | \u001b[0m 0.000143\u001b[0m | \u001b[0m 0.1504  \u001b[0m | \u001b[0m 17.42   \u001b[0m | \u001b[0m 119.2   \u001b[0m | \u001b[0m 72.16   \u001b[0m | \u001b[0m-343.8   \u001b[0m | \u001b[0m 0.9182  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 207.3   \u001b[0m | \u001b[0m 0.9015  \u001b[0m | \u001b[0m 4.888   \u001b[0m | \u001b[0m 4.1     \u001b[0m | \u001b[0m 1.926   \u001b[0m | \u001b[0m 1.801   \u001b[0m | \u001b[0m 0.000619\u001b[0m | \u001b[0m 0.000111\u001b[0m | \u001b[0m 0.1151  \u001b[0m | \u001b[0m 16.53   \u001b[0m | \u001b[0m 112.0   \u001b[0m | \u001b[0m 74.65   \u001b[0m | \u001b[0m-344.8   \u001b[0m | \u001b[0m 0.9123  \u001b[0m |\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m 334.6   \u001b[0m | \u001b[95m 0.9138  \u001b[0m | \u001b[95m 4.136   \u001b[0m | \u001b[95m 3.069   \u001b[0m | \u001b[95m 1.813   \u001b[0m | \u001b[95m 1.29    \u001b[0m | \u001b[95m 0.000704\u001b[0m | \u001b[95m 0.000732\u001b[0m | \u001b[95m 0.2671  \u001b[0m | \u001b[95m 18.73   \u001b[0m | \u001b[95m 119.2   \u001b[0m | \u001b[95m 74.68   \u001b[0m | \u001b[95m-347.1   \u001b[0m | \u001b[95m 0.9321  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 15.76   \u001b[0m | \u001b[0m 0.9723  \u001b[0m | \u001b[0m 5.409   \u001b[0m | \u001b[0m 3.821   \u001b[0m | \u001b[0m 1.124   \u001b[0m | \u001b[0m 1.855   \u001b[0m | \u001b[0m 0.000676\u001b[0m | \u001b[0m 0.000359\u001b[0m | \u001b[0m 0.189   \u001b[0m | \u001b[0m 50.06   \u001b[0m | \u001b[0m 46.28   \u001b[0m | \u001b[0m 89.35   \u001b[0m | \u001b[0m-84.37   \u001b[0m | \u001b[0m 0.9848  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 90.43   \u001b[0m | \u001b[0m 0.9503  \u001b[0m | \u001b[0m 4.78    \u001b[0m | \u001b[0m 4.383   \u001b[0m | \u001b[0m 1.522   \u001b[0m | \u001b[0m 1.837   \u001b[0m | \u001b[0m 0.000163\u001b[0m | \u001b[0m 0.000425\u001b[0m | \u001b[0m 0.1385  \u001b[0m | \u001b[0m 25.44   \u001b[0m | \u001b[0m 123.0   \u001b[0m | \u001b[0m 72.82   \u001b[0m | \u001b[0m-348.1   \u001b[0m | \u001b[0m 0.9822  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 33.81   \u001b[0m | \u001b[0m 0.9737  \u001b[0m | \u001b[0m 3.471   \u001b[0m | \u001b[0m 4.316   \u001b[0m | \u001b[0m 1.198   \u001b[0m | \u001b[0m 1.542   \u001b[0m | \u001b[0m 0.000170\u001b[0m | \u001b[0m 0.000305\u001b[0m | \u001b[0m 0.2157  \u001b[0m | \u001b[0m 17.06   \u001b[0m | \u001b[0m 121.6   \u001b[0m | \u001b[0m 77.53   \u001b[0m | \u001b[0m-353.6   \u001b[0m | \u001b[0m 0.9655  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 166.0   \u001b[0m | \u001b[0m 0.9869  \u001b[0m | \u001b[0m 4.916   \u001b[0m | \u001b[0m 5.744   \u001b[0m | \u001b[0m 1.673   \u001b[0m | \u001b[0m 1.487   \u001b[0m | \u001b[0m 0.000203\u001b[0m | \u001b[0m 0.000690\u001b[0m | \u001b[0m 0.2577  \u001b[0m | \u001b[0m 17.62   \u001b[0m | \u001b[0m 88.07   \u001b[0m | \u001b[0m 117.1   \u001b[0m | \u001b[0m-135.2   \u001b[0m | \u001b[0m 0.9771  \u001b[0m |\n",
      "=====================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, config_data):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.layer_num_actor = int(round(config_data['layer_num_actor'],0))\n",
    "        self.node_num_actor = int(round(config_data['node_num_actor'],0))\n",
    "        self.epochs_actor = int(round(config_data['epochs_actor'],0))\n",
    "        self.layer_num_critic = int(round(config_data['layer_num_critic'],0))\n",
    "        self.node_num_critic = int(round(config_data['node_num_critic'],0))\n",
    "        self.epochs_critic = int(round(config_data['epochs_critic'],0))\n",
    "        \n",
    "        self.learning_rate_actor = config_data['learning_rate_actor']\n",
    "        self.learning_rate_critic = config_data['learning_rate_critic']\n",
    "        self.discount_rate = config_data['discount_rate']\n",
    "        self.smooth_rate = config_data['smooth_rate']\n",
    "        self.penalty = int(round(config_data['penalty'],0))\n",
    "        self.mini_batch_step_size = int(round(config_data['mini_batch_step_size'],0))\n",
    "        self.loss_clipping = config_data['loss_clipping']\n",
    "\n",
    "        self.episode_num = 100\n",
    "        self.moving_avg_size = 20\n",
    "        \n",
    "        self.model_actor = self.build_model_actor()\n",
    "        self.model_critic = self.build_model_critic()\n",
    " \n",
    "        self.states, self.states_next, self.action_matrixs, self.dones, self.action_probs, self.rewards = [],[],[],[],[],[]\n",
    "        self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE = np.zeros((1,1,self.action_size)), np.zeros((1,1,self.value_size))\n",
    "    \n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages, loss_clipping = in_datas[0], in_datas[1], in_datas[2], in_datas[3]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)    \n",
    "                r = new_policy/(old_policy)\n",
    "                \n",
    "                LOSS_CLIPPING = K.mean(loss_clipping)\n",
    "                \n",
    "                loss = -K.minimum(r*advantages, K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)*advantages)\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model_actor(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
    "        input_advantages = Input(shape=(1,self.value_size), name='input_advantages')\n",
    "        input_loss_clipping = Input(shape=(1,self.value_size), name='input_loss_clipping')        \n",
    "        \n",
    "        x = (input_states)\n",
    "        for i in range(1,self.layer_num_actor+1):            \n",
    "            x = Dense(self.node_num_actor, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_advantages], outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate_actor))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_model_critic(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        \n",
    "        x = (input_states)\n",
    "        for i in range(1,self.layer_num_critic+1):\n",
    "            x = Dense(self.node_num_critic, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n",
    "        out_values = Dense(self.value_size, activation='linear', name='output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate_critic),\n",
    "#                       loss='mean_squared_error'\n",
    "                      loss = \"binary_crossentropy\"\n",
    "                     )\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "            \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "            \n",
    "\n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states, self.states_next, self.action_matrixs, self.done, self.action_probs, self.rewards = [],[],[],[],[],[]\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        advantage = np.zeros(self.value_size)\n",
    "        target = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(self.normalize(state),[1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
    "            \n",
    "            action_prob = self.model_actor.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE])\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size) #초기화\n",
    "            action_matrix[action] = 1\n",
    "\n",
    "            state_next, reward, done, none = self.env.step(action)\n",
    "            \n",
    "            state_next_t = np.reshape(self.normalize(state_next),[1, 1, self.state_size])\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "        \n",
    "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
    "            self.states_next.append(np.reshape(state_next_t, [1,self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
    "            self.dones.append(np.reshape(0 if done else 1, [1,self.value_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
    "            self.rewards.append(np.reshape(reward, [1,self.value_size]))\n",
    "            \n",
    "            if(count % self.mini_batch_step_size == 0):\n",
    "                self.train_mini_batch()\n",
    "                self.clear_memory()\n",
    "\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "            \n",
    "        return count, reward_tot\n",
    "    \n",
    "    def make_gae(self, values, values_next, rewards, dones):\n",
    "        delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "        advantages = np.zeros(np.array(values).shape)\n",
    "        targets = np.zeros(np.array(values).shape)\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            delta_adv = rewards[t] + self.discount_rate * values_next[t] * dones[t] - values[t]\n",
    "            delta_tar = rewards[t] + self.discount_rate * values_next[t] * dones[t]\n",
    "            adv = delta_adv + self.smooth_rate *  self.discount_rate * dones[t] * adv\n",
    "            target = delta_tar + self.smooth_rate * self.discount_rate * dones[t] * target\n",
    "            advantages[t] = adv\n",
    "            targets[t] = target\n",
    "        return advantages, targets\n",
    "\n",
    "    def normalize(self, x):\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0: \n",
    "            return x\n",
    "        return x / norm\n",
    "\n",
    "\n",
    "    def train_mini_batch(self):\n",
    "        \n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        states_t = np.array(self.states)\n",
    "        states_next_t = np.array(self.states_next)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "        loss_clipping = [self.loss_clipping for j in range(len(self.states))]\n",
    "        loss_clipping_t = np.reshape(loss_clipping, [len(self.states),1,1])\n",
    "        \n",
    "        values = self.model_critic.predict(states_t)\n",
    "        values_next = self.model_critic.predict(states_next_t)\n",
    "        \n",
    "        advantages, targets = self.make_gae(values, values_next, self.rewards, self.dones)\n",
    "        advantages_t = np.array(advantages)\n",
    "        targets_t = np.array(targets)\n",
    "        \n",
    "        self.model_actor.fit([states_t, action_matrixs_t, advantages_t, loss_clipping_t], [action_probs_t], \n",
    "                             epochs=self.epochs_actor, verbose=0)\n",
    "        self.model_critic.fit(states_t, targets_t, \n",
    "                              epochs=self.epochs_critic, verbose=0)       \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def black_box_function(layer_num_actor, node_num_actor, epochs_actor, \n",
    "                           layer_num_critic, node_num_critic, epochs_critic,\n",
    "                           learning_rate_actor, learning_rate_critic,\n",
    "                           discount_rate, smooth_rate, \n",
    "                           penalty, mini_batch_step_size, loss_clipping\n",
    "                          ):\n",
    "        config_data = {\n",
    "            'layer_num_actor':layer_num_actor,\n",
    "            'node_num_actor':node_num_actor,\n",
    "            'epochs_actor':epochs_actor,\n",
    "            'layer_num_critic':layer_num_critic,\n",
    "            'node_num_critic':node_num_critic,\n",
    "            'epochs_critic':epochs_critic,\n",
    "            \n",
    "            'learning_rate_actor' :learning_rate_actor,\n",
    "            'learning_rate_critic':learning_rate_critic,\n",
    "            'discount_rate'       :discount_rate,\n",
    "            'smooth_rate'       :smooth_rate,\n",
    "            'penalty'             :penalty,\n",
    "            'mini_batch_step_size':mini_batch_step_size,\n",
    "            'loss_clipping'       :loss_clipping\n",
    "        }\n",
    "        agent = Agent(config_data)\n",
    "        agent.train()\n",
    "        return np.mean(agent.reward_list)\n",
    "        \n",
    "    pbounds = {\n",
    "                'layer_num_actor':(1,2),\n",
    "                'node_num_actor':(12,128),\n",
    "                'epochs_actor':(3,6),\n",
    "                'layer_num_critic':(1,2),\n",
    "                'node_num_critic':(12,128),\n",
    "                'epochs_critic':(3,6),\n",
    "\n",
    "                'learning_rate_actor' :(0.0001,0.001),\n",
    "                'learning_rate_critic':(0.0001,0.001),\n",
    "                'discount_rate'       :(0.9,0.99),\n",
    "                'smooth_rate'       :(0.9,0.99),\n",
    "                'penalty'             :(-500,-10),\n",
    "                'mini_batch_step_size':(4,80),\n",
    "                'loss_clipping'       :(0.1,0.3)\n",
    "              }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=black_box_function,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=5,\n",
    "        n_iter=20\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[334.62, 20],\n",
       " [260.99, 7],\n",
       " [207.33, 19],\n",
       " [165.97, 24],\n",
       " [142.99, 5],\n",
       " [112.53, 18],\n",
       " [90.43, 22],\n",
       " [50.81, 11],\n",
       " [42.52, 10],\n",
       " [37.65, 0],\n",
       " [34.92, 4],\n",
       " [33.81, 23],\n",
       " [33.65, 6],\n",
       " [32.31, 2],\n",
       " [31.68, 17],\n",
       " [29.04, 8],\n",
       " [28.45, 16],\n",
       " [28.11, 12],\n",
       " [25.11, 3],\n",
       " [23.1, 1],\n",
       " [23.04, 9],\n",
       " [20.63, 15],\n",
       " [17.65, 14],\n",
       " [15.76, 21],\n",
       " [15.64, 13]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = []\n",
    "i=0\n",
    "for res in optimizer.res:\n",
    "    target_list.append([res[\"target\"], i])\n",
    "    i=i+1\n",
    "target_list.sort(reverse=True)    \n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*result: {'discount_rate': 0.9138068228055699, 'epochs_actor': 4.135796340297432, 'epochs_critic': 3.068824820615902, 'layer_num_actor': 1.8127168005702576, 'layer_num_critic': 1.2899196661865222, 'learning_rate_actor': 0.0007044665544668867, 'learning_rate_critic': 0.0007325323236616151, 'loss_clipping': 0.2671282081035625, 'mini_batch_step_size': 18.73240705651665, 'node_num_actor': 119.19096504720964, 'node_num_critic': 74.68079589490598, 'penalty': -347.12015260105, 'smooth_rate': 0.9321116290822046}\n"
     ]
    }
   ],
   "source": [
    "print(\"*result:\" , optimizer.res[20]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_100: 6\n"
     ]
    }
   ],
   "source": [
    "count_100 = 0\n",
    "for res in optimizer.res:\n",
    "    if(res[\"target\"] >= 100):\n",
    "        count_100 = count_100+1\n",
    "print(\"count_100:\", count_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
